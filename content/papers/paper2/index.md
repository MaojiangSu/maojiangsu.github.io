---
title: "Transformers are Deep Optimizers: Provable In-Context Learning for DeepModel Training"
date: 2025-05-14
tags: ["Transformers", "Deep Learning", "In-Context Learning", "Optimization"]
author: ["Weimin Wu", "Maojiang Su", "Jerry Yao-Chieh Hu", " Zhao Song", "Han Liu"]
description: "This paper investigates the transformer's capability for in-context learning (ICL) to simulate the training process of deep models, providing a provable explicit construction. Published in The International Conference on Machine Learning, 2025."
summary: "This paper investigates the transformer's capability for in-context learning (ICL) to simulate the training process of deep models, providing a provable explicit construction."
cover:
    image: "icl_deep_icml2025_poster.png"
    alt: "Transformers are Deep Optimizers"
    relative: true
editPost:
    URL: "https://github.com/pmichaillat/hugo-website"
    Text: "Published in The International Conference on Machine Learning, 2025."
---

---

##### Download

+ [Paper](6952_Transformers_are_Deep_Opt.pdf)

---

##### Abstract

We investigate the transformerâ€™s capability for incontext learning (ICL) to simulate the training process of deep models. 
Our key contribution is providing a positive example of using a transformer to train a deep neural network by gradient descent in an implicit fashion via ICL.

---

##### Figure 1: Transformers are Deep Optimizers: Provable In-Context Learning for DeepModel Training

![](icl_deep_icml2025_poster.png)
